import copy
from threshold import th
import os
import torch
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
import numpy as np
import os
import sys
from dataconcate import test_map,zero_test_map,train_map
from source_data import source_map
from SR2CNN import getSR2CNN

version='RELEASE'
feature_dim=3*128
num_class=4
model = getSR2CNN(num_class,feature_dim)
#model_path='./complexmodel15.pth'.format(version)
model_path='./feature_extractor1.pth'.format(version)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_semantic_ndarray(data):
    tensor_device = torch.Tensor(data).to(device)
    return model.getSemantic(tensor_device).cpu().numpy()


def calculate_distance(x, transform_matrix):
    return np.sqrt(np.dot(np.dot(x, transform_matrix), x.transpose()))


def RESULT_LOGGER(result_list, message):
    result_list.append('{}\n'.format(message))
    print(message)


def gen_sematic_vec(train_map):
    semantic_center_map = {}
    cov_inv_map = {}
    cov_inv_diag_map = {}
    sigma_identity_map = {}
    distance_map = {}

    for certain_class, train_data in train_map.items():
        raw_output = get_semantic_ndarray(train_data)
        semantic_center_map[certain_class] = np.mean(raw_output, 0)

        covariance_mat = np.cov(raw_output, rowvar=False, bias=True)
        #rows,cols=raw_output.shape
        #covariance_mat = np.identity(cols)
        cov_inv_map = np.linalg.pinv(covariance_mat)

        cov_inv_diag_mat = np.diagflat(1 / (covariance_mat.diagonal()))
        cov_inv_diag_mat[cov_inv_diag_mat == np.inf] = 0.0
        cov_inv_diag_map[certain_class] = cov_inv_diag_mat
        sigma = np.mean(np.diagflat(covariance_mat.diagonal()))
        sigma_identity_map[certain_class] = 1 / sigma * np.eye(covariance_mat.shape[0])

    distance_map['Maha'] = cov_inv_map
    distance_map['MahaDiag'] = cov_inv_diag_map
    distance_map['SigmaEye'] = sigma_identity_map

    return semantic_center_map, distance_map


def classify_evol(transform_map, semantic_center_map, semantic_vector, coef):
    predicted_label = -1
    min_dist = float('inf')
    min_dist_recorded = float('inf')
    dists_known_I = []
    dists_known =[]
    if_known = False
    for certain_class in range(num_class):
        semantic_center = semantic_center_map[certain_class]
        dist = calculate_distance(semantic_vector - semantic_center, transform_map[certain_class])
        dists_known.append(dist)
        eyeMat = np.eye(semantic_center_map[certain_class].shape[0])
        dist_I = calculate_distance(semantic_vector - semantic_center, eyeMat)
        dists_known_I.append(dist_I)

        if dist < th[certain_class]:
            if dist < min_dist:
                min_dist = dist
                predicted_label = certain_class
        #if dist < max_dist * coef:
        if_known = True
    if not if_known:
        predicted_label = -1
    return predicted_label

def confusion_matrix(true_labels, pred_labels, num_class):
    # Initialize an empty matrix with shape (num_class + 1, num_class + 1)
    # The last row and column are for the unknown class
    matrix = np.zeros((num_class + 1, num_class + 1),dtype=int)
    # Loop through each pair of true and predicted labels
    for true, pred in zip(true_labels, pred_labels):
        # Increment the corresponding cell in the matrix by 1
        matrix[true, pred] += 1
    # Return the matrix
    return matrix
def classification_metrics(matrix, num_class):
    # Initialize an empty dictionary to store the metrics
    metrics = {}
    # Loop through each class
    for i in range(num_class + 1):
        # Calculate the true positive, false positive, false negative and true negative for this class
        tp = matrix[i, i]
        fp = matrix[:, i].sum() - tp
        fn = matrix[i, :].sum() - tp
        tn = matrix.sum() - tp - fp - fn
        #tp_s = matrix[0, 0] + matrix[1, 1] + matrix[2, 2] + matrix[3,3]+matrix[4,4]+matrix[5,5]
        #tp_a = matrix[0, 0] + matrix[1, 1] + matrix[2, 2] + matrix[3,3] +matrix[4,4]+matrix[5,5]+matrix[6,6]
        tp_s = matrix[0, 0] + matrix[1, 1] + matrix[2, 2] + matrix[3,3]
        tp_a = matrix[4,4]
        # Calculate the accuracy, precision and recall for this class
        seen_accuracy = tp_s/2000
        unknown_accuracy = tp_a/500
        #accuracy = tp/ (tp+fp+fn)
        precision = tp / (tp + fp) if tp + fp > 0 else 0
        recall = tp / (tp + fn) if tp + fn > 0 else 0
        F1 = (2*precision*recall) / (precision+recall) if precision+recall > 0 else 0
        # Store the metrics in the dictionary with the class name as the key
        metrics[f"class_{i}"] = {"F1": F1,"seen_accuracy ": seen_accuracy,'acc':unknown_accuracy}
    # Return the metrics dictionary
    return metrics
def cal_acc_evol(train_map, test_map, unknown_test_map, distance='MahaDiag'):
    semantic_center_map_origin, distance_map = gen_sematic_vec(train_map)


    tackled_test_data = np.concatenate((*(test_map.values()),), 0)
    tackled_label = np.concatenate(
        (*map(lambda x: np.full([x[1].shape[0]], x[0], dtype=np.int64), test_map.items()),), 0)
    tackled_unknown_test_data = np.concatenate((*(unknown_test_map.values()),), 0)
    tackled_unknown_label = np.concatenate(
        (*map(lambda x: np.full([x[1].shape[0]], x[0], dtype=np.int64), unknown_test_map.items()),), 0)
    test_samples = np.concatenate((tackled_test_data, tackled_unknown_test_data), 0)
    test_labels = np.concatenate((tackled_label, tackled_unknown_label), 0)
    predicted_semantics = get_semantic_ndarray(test_samples)
    transform_map = distance_map[distance]

    #coef = 30
    coef = 0.05
    semanticMap = copy.deepcopy(semantic_center_map_origin)
    pred_labels = []
    for certain_class, predicted_semantic in zip(test_labels, predicted_semantics):
        predicted_label = classify_evol(transform_map, semanticMap, predicted_semantic, coef)
        pred_labels.append(predicted_label)
    matrix = confusion_matrix(test_labels,pred_labels,num_class)
    metrics = classification_metrics(matrix,num_class)
    print(matrix)
    print(metrics)



if __name__ == '__main__':


    model_paths = []
    if len(sys.argv) > 1:
        path = sys.argv[1]
        if os.path.isdir(path):
            model_paths = [os.path.join(path, x) for x in os.listdir(path)]
            version = list(filter(lambda x: not x == '', path.split('/')))[-1]
        elif os.path.isfile(path):
            model_paths.append(path)
    else:
        model_paths.append(model_path)

    model.to(device)

    for model_path in model_paths:
        print('Loading model from {}'.format(model_path))
        #checkpoint = torch.load(model_path)
        model.load_state_dict(torch.load(model_path, map_location=device))
        with torch.no_grad():
            model.eval()
            print('ZSL evaluation')
            cal_acc_evol(train_map, test_map, zero_test_map)

    print('end')